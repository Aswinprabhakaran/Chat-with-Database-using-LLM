{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aswinprabhakaran/Generative_AI/blob/main/rag_and_ragas_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Interactive Exploration of RAG and the RAGAS Framework:</b>\n",
        "\n",
        "Hey there! Today, I wanted to put together a quick notebook to show you how to both build and evaluate a RAG system (a cooler than average one I think...). We will start by reviewing RAG (super quick) before diving into some basics and keypoints from the RAGAS paper.\n",
        "\n",
        "---\n",
        "\n",
        "<b><a href=\"https://arxiv.org/abs/2309.15217\">PAPER: RAGAS: Automated Evaluation of Retrieval Augmented Generation</a></b>\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://github.com/explodinggradients/ragas/raw/main/docs/_static/imgs/logo.png\"></center>\n",
        "\n",
        "---\n",
        "\n",
        "<i>Github Link: <b><a href=\"https://github.com/explodinggradients/ragas\">explodinggradients/ragas</a></b></i>\n",
        "<i>Langchain Blog: <b><a href=\"https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/\">Evaluating RAG Pipelines with Ragas and Langsmith</a></b></i>\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "OSv-zDMdL9R4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## <b>RAG OVERVIEW (SORTA ELI5)</b>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### <b>Understanding RAG</b>\n",
        "\n",
        "So... I was going to do a big thing here. But there are SO MANY good resources on this already available.\n",
        "\n",
        "For now I will simply give the below ELI5ish version and recommned you check out this <b>brilliant, recent guide to Retrieval Augmented Generation from <a href=\"https://www.lakera.ai/\">lakera.ai</a>:</b>\n",
        "* https://www.lakera.ai/blog/retrieval-augmented-generation\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://assets-global.website-files.com/651c34ac817aad4a2e62ec1b/655664de69b30a6d00f0960c_gaJkRvUmWHsWtnAGlNtjQJYhSzHvUwZHvV7nDU3kQJ6EyEI1C4v6HRysXIw28UlXK3QT4yU0rgTD7v1cUgbl5nB71emE5vqz9Y0VlvLjg10BgaLcOvI4Zauu9AKU6EKWN5rIwIKPs8CSYd0CiX2Gg5g.png\" width=100%>\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "#### <b>THE BASICS</b>\n",
        "\n",
        "Retrieval Augmented Generation (RAG) is like giving a super boost to language models we use every day.\n",
        "* Think of it as hooking up these smart models with a speedy, always-updated information buddy.\n",
        "* This buddy, or the external retrieval system, helps the model to not only be super smart but also super relevant and up-to-date.\n",
        "\n",
        "Let's face it, even the smartest language models can sometimes be a bit behind the times *(ehem, who's the CEO of OpenAI ChatGPT?)*...\n",
        "* RAG changes things... with it we get fresh, dynamic info from external sources\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "#### <b>HOW IT WORKS?</b>\n",
        "\n",
        "1. **The Retriever**: This part dives into a vast ocean of data, fishing out the pieces that matter for your question.\n",
        "2. **The Generator**: Then comes the clever language model, taking this fresh catch of information and turning it into a clear, accurate answer.\n",
        "\n",
        "And it's not just about finding words that match. The retriever uses some cool tricks, like cosine similarity, to understand the *meaning* (semantic meaning) behind your question and find info that is relevant to what you're looking for.\n",
        "* As a brief aside, embedding lookup is like turning words and documents into numerical representation (called embeddings) and then finding other words/documents whose numerical represnentation is as close as possible – it's like a high-tech treasure hunt for information!\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "#### <b>PROS AND CONS</b>\n",
        "\n",
        "**The pros:**\n",
        "* RAG is like a quick learner, reducing the need for heavy-duty training, and it's like a universal adapter, pulling from all sorts of knowledge sources.\n",
        "* Because retrieval is fast and models don't have to be retrained, RAG is quick to iterate with and reasonably scalable, growing and changing with your needs.\n",
        "\n",
        "**The cons:**\n",
        "* RAG can sometimes get a bit imaginative, creating convincing yet not entirely accurate responses (we call these 'hallucinations').\n",
        "* It also faces challenges in managing its vast knowledge database and keeping biases at bay.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "#### <b>IN A NUTSHELL</b>\n",
        "\n",
        "RAG is a super cool (and very important) way to make smart language models even smarter and more relevant to what you need right now!\n",
        "\n",
        "<br>\n",
        "\n",
        "<b><center>You will, 100% of the time, need to know this to build capable LLM based systems<center></b>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mRb_64CxJhfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## <b>RAGAS RESEARCH PAPER OVERVIEW (SORTA ELI5)</b>\n",
        "\n",
        "---\n",
        "\n",
        "### <b>Understanding RAGAS: A Friendly Guide to Evaluating AI's Smart Answers</b>\n",
        "\n",
        "\n",
        "**A few basics regarding RAGAS.**\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **What's RAGAS All About?**\n",
        "RAGAS stands for **Retrieval Augmented Generation Assessment**. Now, that's a mouthful, but it's actually quite straightforward:\n",
        "\n",
        "- **Retrieval Augmented Generation (RAG)**: As discussed above, this is like giving our AI a library card. It can pull books (data) off the shelf to answer your questions more accurately.\n",
        "\n",
        "- **Why RAGAS?** Well... how do we know if the AI is really making sense with the data it fetches? That's where RAGAS comes in. It's like a scorecard for our AI's homework. Note that we get more than just accuracy too, we can actually see an evaluation of different categories that gives us insight into what parts of RAG went well and what parts... did not.\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://blog.langchain.dev/content/images/size/w1000/2023/08/image-21.png\">\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "#### **The ThrFouree Report Card Subjects**\n",
        "RAGAS grades our AI in three key areas:\n",
        "\n",
        "1. **Faithfulness**: Is our AI telling truths or fairy tales? Basically, are the answers based on the data it found, or is it making stuff up?\n",
        "\n",
        "2. **Answer Relevance**: Did the AI really get what you were asking? It's like asking for an apple and making sure you don't get an orange instead.\n",
        "\n",
        "3. **Context Relevance/Precision**: Did the AI pick the right books? It's no good if it fetches a cookbook when you asked about space rockets!\n",
        "\n",
        "4. **Context Recall**: Can the AI get all the relevant info required to answer things?\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "#### **Why Should We Care?**\n",
        "1. **DIY Grading**: With RAGAS, you don't always need humans to check the AI's work. Handy, right?\n",
        "2. **Beyond Just Grammar**: It's not just about fancy words; it's about making sense.\n",
        "3. **AI Developers'**: If you're building an AI, RAGAS is like your helpful assistant, ensuring your AI is on point.\n",
        "4. **No Reference Needed**: You don't always need an answer key. RAGAS can grade on the go!\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "So, that's RAGAS in a nutshell.\n",
        "\n",
        "It's an interesting new tool aimed at solving one of the hardest new problems in AI, evaluating LLMs and their outputs.\n",
        "\n",
        "<br>\n",
        "\n",
        "Let's dive into the code!"
      ],
      "metadata": {
        "id": "m_BJi9CYXkv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## **IMPORTS**\n",
        "\n",
        "---\n",
        "\n",
        "<center><b>For this to work you will need to either have your enviornment file in the exact same place as me... unlikely... or you need to upload it when prompted to. This has to have an api key for openai for anything to work...</b></center>\n",
        "\n",
        "---\n",
        "\n",
        "For the most part, these are overkill. Beyond a few (ragas, openai, langchain, etc.) most are just superfluous carry-over from other notebooks I've written\n",
        "\n",
        "This will take a minute or two...\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "75HWjpCqLrtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is for environment variables...\n",
        "env_path = \"/content/drive/MyDrive/secrets/.env\" # my weird path\n",
        "if not os.path.isfile(env_path):\n",
        "    env_path = \".env\" # your probable path or the one to upload to.\n",
        "    if not os.path.isfile(env_path):\n",
        "        from google.colab import files\n",
        "        print(\"Please upload your environment file. It should either be env.txt or .env\")\n",
        "        uploaded = files.upload()\n",
        "        if not os.path.isfile(\".env\"):\n",
        "            !mv env.txt .env\n",
        "        if not os.path.isfile(\".env\"):\n",
        "            raise ValueError(\"Something went wrong... please try again...\")"
      ],
      "metadata": {
        "id": "AFM84XpeOprZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcoScx4gIhc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb7b4598-76b8-4a3c-849f-b2b5a691b1c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "... PIP INSTALLS STARTING ...\n",
            "\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.1/275.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting ragas\n",
            "  Downloading ragas-0.0.20-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ragas) (1.23.5)\n",
            "Collecting datasets (from ragas)\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken (from ragas)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from ragas) (0.0.339)\n",
            "Requirement already satisfied: openai>1 in /usr/local/lib/python3.10/dist-packages (from ragas) (1.3.3)\n",
            "Collecting pysbd>=0.3.4 (from ragas)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas) (1.5.8)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>1->ragas) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (4.5.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets->ragas)\n",
            "  Downloading pyarrow_hotfix-0.5-py3-none-any.whl (7.8 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->ragas)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.4.1)\n",
            "Collecting multiprocess (from datasets->ragas)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.19.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (2.0.23)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (0.6.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (0.0.66)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (8.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->ragas) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai>1->ragas) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai>1->ragas) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai>1->ragas) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragas) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragas) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets->ragas) (3.13.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain->ragas) (2.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->ragas) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->ragas) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->ragas) (1.0.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
            "Installing collected packages: pysbd, pyarrow-hotfix, dill, tiktoken, multiprocess, datasets, ragas\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.5 pysbd-0.3.4 ragas-0.0.20 tiktoken-0.5.1\n",
            "\n",
            "... PIP INSTALLS COMPLETE ...\n",
            "\n",
            "\n",
            "... IMPORTS STARTING ...\n",
            "\n",
            "\n",
            "\tVERSION INFORMATION\n",
            "\t\t– TORCH VERSION: 2.1.0+cu118\n",
            "\t\t– NUMPY VERSION: 1.23.5\n",
            "\t\t– SKLEARN VERSION: 1.2.2\n",
            "\t\t– SCIPY VERSION: 1.11.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\t– MATPLOTLIB VERSION: 3.7.1\n",
            "\n",
            "... SEEDING FOR REPRODUCIBILITY ...\n",
            "\n",
            "\n",
            "... IMPORTS COMPLETE ...\n",
            "\n",
            "\n",
            "AUTHENTICATED: True\n"
          ]
        }
      ],
      "source": [
        "# Pip installs go here... if none than comment out\n",
        "\n",
        "\n",
        "print(\"\\n... PIP INSTALLS STARTING ...\\n\")\n",
        "!pip install -q --upgrade duckduckgo-search # for rag\n",
        "!pip install -q --upgrade google-search-results # for rag\n",
        "!pip install -q wikipedia # for rag\n",
        "# !pip install -q --upgrade pinecone-client\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q unstructured # for website scraping\n",
        "!pip install -q --upgrade langchain\n",
        "!pip install -q --upgrade openai\n",
        "!pip install -q --upgrade python-dotenv\n",
        "\n",
        "!pip install -q ragas\n",
        "print(\"\\n... PIP INSTALLS COMPLETE ...\\n\")\n",
        "\n",
        "print(\"\\n... IMPORTS STARTING ...\\n\")\n",
        "print(\"\\n\\tVERSION INFORMATION\")\n",
        "\n",
        "# Machine Learning and Data Science Imports (basics)\n",
        "import torch; print(f\"\\t\\t– TORCH VERSION: {torch.__version__}\");\n",
        "import pandas as pd; pd.options.mode.chained_assignment = None; pd.set_option('display.max_columns', None);\n",
        "import numpy as np; print(f\"\\t\\t– NUMPY VERSION: {np.__version__}\");\n",
        "import sklearn; print(f\"\\t\\t– SKLEARN VERSION: {sklearn.__version__}\");\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import scipy; print(f\"\\t\\t– SCIPY VERSION: {scipy.__version__}\");\n",
        "\n",
        "# For GPU usage in cuda/hf/torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# LLM Utility Imports\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "# import pinecone\n",
        "\n",
        "# Built-In Imports (mostly don't worry about these)\n",
        "from collections import Counter\n",
        "from typing import List, Union\n",
        "from datetime import datetime\n",
        "from zipfile import ZipFile\n",
        "from glob import glob\n",
        "import warnings\n",
        "import requests\n",
        "import hashlib\n",
        "import imageio\n",
        "import IPython\n",
        "import sklearn\n",
        "import urllib\n",
        "import zipfile\n",
        "import pickle\n",
        "import random\n",
        "import shutil\n",
        "import string\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import gzip\n",
        "import ast\n",
        "import sys\n",
        "import io\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "\n",
        "# Visualization Imports (overkill)\n",
        "import matplotlib; print(f\"\\t\\t– MATPLOTLIB VERSION: {matplotlib.__version__}\");\n",
        "from PIL import Image, ImageEnhance; Image.MAX_IMAGE_PIXELS = 5_000_000_000;\n",
        "from tqdm.notebook import tqdm; tqdm.pandas();\n",
        "from IPython.display import HTML\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import plotly\n",
        "import PIL\n",
        "import cv2\n",
        "\n",
        "\n",
        "def seed_it_all(seed=7, seed_tf=False, seed_np=False):\n",
        "    \"\"\" Attempt to be Reproducible \"\"\"\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    if seed_np:\n",
        "        np.random.seed(seed)\n",
        "    if seed_tf:\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "print(\"\\n... SEEDING FOR REPRODUCIBILITY ...\")\n",
        "seed_it_all(seed_np=True)\n",
        "\n",
        "print(\"\\n\\n... IMPORTS COMPLETE ...\\n\")\n",
        "\n",
        "AUTHENTICATED = load_dotenv(dotenv_path=env_path)\n",
        "# pinecone.init()\n",
        "print(\"\\nAUTHENTICATED:\", AUTHENTICATED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Langchain Imports\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.tools import DuckDuckGoSearchResults\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from langchain.tools.base import StructuredTool\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import BaseChatPromptTemplate\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
        "from langchain.utilities import SerpAPIWrapper\n",
        "from langchain.schema import Document, AgentAction, AgentFinish, HumanMessage\n",
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
        "from langchain.retrievers import WikipediaRetriever\n",
        "\n",
        "# Ragas Imports\n",
        "import ragas\n",
        "from ragas.metrics import faithfulness, answer_relevancy, context_relevancy, context_recall\n",
        "from ragas.langchain import RagasEvaluatorChain"
      ],
      "metadata": {
        "id": "Vh7Ux0UfQU4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## <b>LANGCHAIN BASICS</b>\n",
        "\n",
        "---\n",
        "\n",
        "I'm not going to explain this much... but the basics are that we are using the langchain library to instantiate an embedding model (ada2) and a GPT4 chat model that can take in a prompt with N number of injectable kwargs. For fun we also stream the output\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "I show a few demos below the instantiation."
      ],
      "metadata": {
        "id": "Q3gJz0mHW1Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_model(model_name=\"text-embedding-ada-002\"):\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model=model_name,\n",
        "    )\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "### EXAMPLE USAGE FOR CHAT MODEL\n",
        "def create_openai_model(model_name=\"gpt-4\", temperature=0.7, streaming=True, streaming_cb=None):\n",
        "    if streaming_cb:\n",
        "        callbacks=[streaming_cb]\n",
        "    else:\n",
        "        callbacks=[StreamingStdOutCallbackHandler()]\n",
        "\n",
        "    chat = ChatOpenAI(\n",
        "        model=model_name,\n",
        "        temperature=temperature,\n",
        "        streaming=streaming,\n",
        "        callbacks=callbacks if streaming else None\n",
        "    )\n",
        "    return chat\n",
        "\n",
        "chat_model = create_openai_model()\n",
        "embedding_model = create_embedding_model()\n",
        "prompt = \"What are ten words that are related to the word {word}. Format the output as a newline seperated list.\"\n",
        "user_input = input(\"What word do you want to find similar/related words for?\\n\")\n",
        "chain = LLMChain(llm=chat_model, prompt=PromptTemplate.from_template(prompt))\n",
        "\n",
        "print(f\"\\n\\nLLM Output for the Prompt: {prompt.format(word=user_input)}\\n\")\n",
        "output = chain.run(word=user_input)\n",
        "output_words = output.split()\n",
        "output_embeddings = np.array(embedding_model.embed_documents([user_input,]+output_words))\n",
        "input_word_embeddings, output_word_embeddings = output_embeddings[:1, :], output_embeddings[1:, :]\n",
        "print(f\"\\n\\nINPUT WORD EMBEDDINGS SHAPE: {input_word_embeddings.shape}\")\n",
        "print(f\"OUTPUT WORD EMBEDDINGS SHAPE: {output_word_embeddings.shape}\")\n",
        "print(\"\\nCOSINE SIMILARITY BETWEEN INPUT AND OUTPUT VECTORS\")\n",
        "similarities = cosine_similarity(input_word_embeddings, output_word_embeddings)\n",
        "print(similarities)\n",
        "\n",
        "print(\"\\nTOP 3 MOST SIMILAR WORDS TO THE INPUT WORD:\")\n",
        "words_sorted_by_cos_sim = sorted(zip(output_words, similarities[0]), key=lambda x: x[1], reverse=True)  # Sort by float value in descending order\n",
        "print(words_sorted_by_cos_sim[:3])\n",
        "\n",
        "print(\"\\nTOP 3 LEAST SIMILAR WORDS TO THE INPUT WORD:\")\n",
        "print(words_sorted_by_cos_sim[-3:])"
      ],
      "metadata": {
        "id": "XeCZFeA0SnXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7234a7e5-b407-43c3-84ed-7c0f1820b831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What word do you want to find similar/related words for?\n",
            "Sour\n",
            "\n",
            "\n",
            "LLM Output for the Prompt: What are ten words that are related to the word Sour. Format the output as a newline seperated list.\n",
            "\n",
            "Acidic\n",
            "Bitter\n",
            "Tart\n",
            "Vinegary\n",
            "Sharp\n",
            "Pungent\n",
            "Fermented\n",
            "Unpleasant\n",
            "Citrus\n",
            "Astringent\n",
            "\n",
            "INPUT WORD EMBEDDINGS SHAPE: (1, 1536)\n",
            "OUTPUT WORD EMBEDDINGS SHAPE: (10, 1536)\n",
            "\n",
            "COSINE SIMILARITY BETWEEN INPUT AND OUTPUT VECTORS\n",
            "[[0.87094549 0.89956217 0.86892705 0.86245806 0.82755925 0.86238088\n",
            "  0.8525818  0.84513514 0.84105312 0.8172916 ]]\n",
            "\n",
            "TOP 3 MOST SIMILAR WORDS TO THE INPUT WORD:\n",
            "[('Bitter', 0.8995621688326766), ('Acidic', 0.8709454911752689), ('Tart', 0.8689270529066335)]\n",
            "\n",
            "TOP 3 LEAST SIMILAR WORDS TO THE INPUT WORD:\n",
            "[('Citrus', 0.8410531187551804), ('Sharp', 0.8275592531381349), ('Astringent', 0.8172916004881715)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## <b>DATA SOURCE(S)</b>\n",
        "\n",
        "---\n",
        "\n",
        "For our experiments I will be using duckduckgo to return a number of webpages related to the user's search which we will then scrape and split. We will also utilize wikipedia search.\n",
        "\n",
        "We will create a small FAISS vectorstore on the fly and use that for RAG. This is probably more dynamic than you've seen most examples but I think it's fun... and this is my notebook... soo..."
      ],
      "metadata": {
        "id": "dQQaZoJSZDQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## <b>THE RETRIEVER</b>\n",
        "\n",
        "---\n",
        "\n",
        "Our retriever will be retrieving from the datasources above. I will use langchain to wrap most of this up into a handy 'tool' that we can use for evaluation (and later we can use it to make an Agent)"
      ],
      "metadata": {
        "id": "SyafLyf2SP5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_to_vs(query, ddg_wrapper=None, wiki_retriever=None, embedding_model=None, n_results=4):\n",
        "    \"\"\"\n",
        "    Retrieves and embeds web and Wikipedia documents related to a given query into a FAISS vector store.\n",
        "\n",
        "    This function performs several steps to gather and process information related to a given query:\n",
        "    1. Executes a DuckDuckGo search for the query.\n",
        "    2. Retrieves URLs from the top search results.\n",
        "    3. Scrapes content from these webpages.\n",
        "    4. Splits the webpage contents into Langchain documents.\n",
        "    5. Filters out documents that are too small (less than 100 characters).\n",
        "    6. Fetches relevant Wikipedia documents based on the query.\n",
        "    7. Embeds all gathered documents into a FAISS vector store for further processing.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query to retrieve and process documents for.\n",
        "        ddg_wrapper (DuckDuckGoSearchAPIWrapper, optional): Wrapper for DuckDuckGo searches.\n",
        "        wiki_retriever (WikipediaRetriever, optional): Retriever for Wikipedia search.\n",
        "        embedding_model (Model, optional): The model to use for embedding documents.\n",
        "        n_results (int, optional): The number of search results to retrieve from DuckDuckGo. Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "    - FAISS VectorStore: A FAISS vector store containing the embedded documents.\n",
        "\n",
        "    Notes:\n",
        "    - The function tries to retrieve relevant Wikipedia documents but will print a warning and continue if this fails.\n",
        "    - The default wrappers for DuckDuckGo and Wikipedia can be replaced by providing custom instances via `ddg_wrapper` and `embedding_model` parameters.\n",
        "    - The function uses a progress bar to show the loading and splitting of webpage documents.\n",
        "    \"\"\"\n",
        "\n",
        "    if wiki_retriever is None:\n",
        "        wiki_retriever = WikipediaRetriever()\n",
        "\n",
        "    if ddg_wrapper is None:\n",
        "        ddg_wrapper = DuckDuckGoSearchAPIWrapper(max_results=n_results)\n",
        "\n",
        "    if embedding_model is None:\n",
        "        embedding_model = create_embedding_model()\n",
        "\n",
        "    docs = UnstructuredURLLoader(\n",
        "        [x['link'] for x in ddg_wrapper.results(query, num_results=n_results)],\n",
        "        show_progress_bar=True, headers={\"User-Agent\": \"value\"}\n",
        "    ).load_and_split()\n",
        "    docs = [x for x in docs if len(x.page_content)>100]\n",
        "\n",
        "    # I noticed this fails occasionally\n",
        "    try:\n",
        "        docs += wiki_retriever.get_relevant_documents(query=query)\n",
        "    except:\n",
        "        print(\"\\n[WARNING] retrieving from wikipedia failed [/WARNING]\\n\")\n",
        "        pass\n",
        "\n",
        "    vs = FAISS.from_documents(docs, embedding_model)\n",
        "    return vs\n",
        "\n",
        "# # I'm abstracting this as a tool because after I will create a basic agent with it\n",
        "# retrieval_tool = Tool(\n",
        "#     name=\"Retrieval\",\n",
        "#     func=query_to_vs,\n",
        "#     description=\"useful for whenever you need information to help answer the user's question\",\n",
        "# )\n"
      ],
      "metadata": {
        "id": "U6l1Ze-daF4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## <b>GENERATION</b>\n",
        "\n",
        "---\n",
        "\n",
        "For this we could easily use the RetrievalQA chain by langchain... let's see what that looks like...\n",
        "\n",
        "Note... this may do some install stuff the first time w.r.t. NLTK\n",
        "\n",
        "\n",
        "Full prompt from the depths of langchain is below... pretty simple really...\n",
        "\n",
        "```\n",
        "# print(\"\\n\".join([x.prompt.template for x in qa_chain.combine_documents_chain.llm_chain.prompt.messages]))\n",
        "\n",
        "QARetrieval Prompt:\n",
        "\n",
        "Use the following pieces of context to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "----------------\n",
        "{context}\n",
        "{question}\n",
        "```"
      ],
      "metadata": {
        "id": "7tb3NdNpSOiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the llm (feel free to change the temp)\n",
        "TEMP = 0.5\n",
        "llm = create_openai_model(temperature=TEMP)\n",
        "\n",
        "# Pick a question\n",
        "user_provided_question = input(\"What do you want to ask?\\n>>> \")\n",
        "\n",
        "# Instantiate the chain and query it\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=query_to_vs(user_provided_question).as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "result = qa_chain({\"query\": user_provided_question})\n",
        "\n",
        "# Let's see the whole object... the streamed result should also be above\n",
        "print(\"\\n\\nFULL RESULT OBJECT:\\n\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21wHErGcTHG1",
        "outputId": "abc61d8f-e603-47a6-810f-f1494f3bc074"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What do you want to ask?\n",
            ">>> What's up with Broadcom and VMware?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:05<00:00,  1.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broadcom Inc. and VMware, Inc. are in the process of a business combination transaction. They have received legal merger clearance in several countries and there is no legal impediment to closing under U.S. merger regulations. The companies expect the acquisition of VMware by Broadcom to close soon. VMware stockholders have had the opportunity to choose whether they wish to receive cash or shares of Broadcom common stock in exchange for their shares of VMware common stock. The transaction is subject to certain conditions and risks, and the companies have advised reading the proxy statement/prospectus for more detailed information.\n",
            "\n",
            "FULL RESULT OBJECT:\n",
            "\n",
            "{'query': \"What's up with Broadcom and VMware?\", 'result': 'Broadcom Inc. and VMware, Inc. are in the process of a business combination transaction. They have received legal merger clearance in several countries and there is no legal impediment to closing under U.S. merger regulations. The companies expect the acquisition of VMware by Broadcom to close soon. VMware stockholders have had the opportunity to choose whether they wish to receive cash or shares of Broadcom common stock in exchange for their shares of VMware common stock. The transaction is subject to certain conditions and risks, and the companies have advised reading the proxy statement/prospectus for more detailed information.', 'source_documents': [Document(page_content='About Broadcom\\n\\nBroadcom Inc. (NASDAQ: AVGO), a Delaware corporation headquartered in San Jose, CA, is a global technology leader that designs, develops and supplies a broad range of semiconductor and infrastructure software solutions. Broadcom’s category-leading product portfolio serves critical markets including data center, networking, enterprise software, broadband, wireless, storage and industrial. Our solutions include data center networking and storage, enterprise, mainframe and cybersecurity software focused on automation, monitoring and security, smartphone components, telecoms and factory automation.\\n\\nAbout VMware\\n\\nVMware is a leading provider of multi-cloud services for all apps, enabling digital innovation with enterprise control. As a trusted foundation to accelerate innovation, VMware software gives businesses the flexibility and choice they need to build the future. Headquartered in Palo Alto, California, VMware is committed to building a better future through the company’s 2030 Agenda. For more information, please visit www.VMware.com/company.\\n\\nCautionary Statement Regarding Forward-Looking Statements\\n\\nThis communication relates to a proposed business combination transaction between Broadcom and VMware. This communication includes forward-looking statements within the meaning of Section 21E of the U.S. Securities Exchange Act of 1934, as amended, and Section 27A of the U.S. Securities Act of 1933, as amended. These forward-looking statements include but are not limited to statements that relate to the anticipated closing date of the proposed transaction. These forward-looking statements are identified by words such as \"will,\" \"expect,\" \"believe,\" \"anticipate,\" \"estimate,\" \"should,\" \"intend,\" \"plan,\" \"potential,\" \"predict,\" \"project,\" \"aim,\" and similar words or phrases. These forward-looking statements are based on current expectations and beliefs of Broadcom and VMware management and current market trends and conditions.\\n\\nThese forward-looking statements involve risks and uncertainties that are outside Broadcom’s and VMware’s control and may cause actual results to differ materially from those contained in forward-looking statements, including but not limited to: business disruption following the announcement and closing of the proposed transaction; difficulties in retaining and hiring key personnel and employees due to the proposed transaction and business combination; the diversion of management time on transaction-related issues; the satisfaction of the conditions precedent to completion of the proposed transaction, including the ability to secure regulatory approvals on the terms expected, at all or in a timely manner; the disruption of current plans and operations; the outcome of legal proceedings related to the Transaction; the ability to complete the proposed transaction on a timely basis or at all; the ability to successfully integrate VMware’s operations; cyber-attacks, information security and data privacy; global political and economic conditions, including cyclicality in the semiconductor industry and in Broadcom’s other target markets, rising interest rates, the impact of inflation and challenges in manufacturing and the global supply chain; the impact of public health crises, such as pandemics (including COVID-19) and epidemics and any related company or government policies and actions to protect the health and safety of individuals or government policies or actions to maintain the functioning of national or global economies and markets; and events and trends on a national, regional and global scale, including those of a political, economic, business, competitive and regulatory nature.', metadata={'source': 'https://finance.yahoo.com/news/broadcom-vmware-pending-transaction-090200559.html'}), Document(page_content='About Broadcom\\n\\nBroadcom Inc. (NASDAQ: AVGO), a Delaware corporation headquartered in San Jose, CA, is a global technology leader that designs, develops and supplies a broad range of semiconductor and infrastructure software solutions. Broadcom’s category-leading product portfolio serves critical markets including data center, networking, enterprise software, broadband, wireless, storage and industrial. Our solutions include data center networking and storage, enterprise, mainframe and cybersecurity software focused on automation, monitoring and security, smartphone components, telecoms and factory automation.\\n\\nAbout VMware\\n\\nVMware is a leading provider of multi-cloud services for all apps, enabling digital innovation with enterprise control. As a trusted foundation to accelerate innovation, VMware software gives businesses the flexibility and choice they need to build the future. Headquartered in Palo Alto, California, VMware is committed to building a better future through the company’s 2030 Agenda. For more information, please visit www.VMware.com/company.\\n\\nCautionary Statement Regarding Forward-Looking Statements\\n\\nThis communication relates to a proposed business combination transaction between Broadcom and VMware. This communication includes forward-looking statements within the meaning of Section 21E of the U.S. Securities Exchange Act of 1934, as amended, and Section 27A of the U.S. Securities Act of 1933, as amended. These forward-looking statements include but are not limited to statements that relate to the anticipated closing date of the proposed transaction. These forward-looking statements are identified by words such as “will,” “expect,” “believe,” “anticipate,” “estimate,” “should,” “intend,” “plan,” “potential,” “predict,” “project,” “aim,” and similar words or phrases. These forward-looking statements are based on current expectations and beliefs of Broadcom and VMware management and current market trends and conditions.\\n\\nThese forward-looking statements involve risks and uncertainties that are outside Broadcom’s and VMware’s control and may cause actual results to differ materially from those contained in forward-looking statements, including but not limited to: business disruption following the announcement and closing of the proposed transaction; difficulties in retaining and hiring key personnel and employees due to the proposed transaction and business combination; the diversion of management time on transaction-related issues; the satisfaction of the conditions precedent to completion of the proposed transaction, including the ability to secure regulatory approvals on the terms expected, at all or in a timely manner; the disruption of current plans and operations; the outcome of legal proceedings related to the Transaction; the ability to complete the proposed transaction on a timely basis or at all; the ability to successfully integrate VMware’s operations; cyber-attacks, information security and data privacy; global political and economic conditions, including cyclicality in the semiconductor industry and in Broadcom’s other target markets, rising interest rates, the impact of inflation and challenges in manufacturing and the global supply chain; the impact of public health crises, such as pandemics (including COVID-19) and epidemics and any related company or government policies and actions to protect the health and safety of individuals or government policies or actions to maintain the functioning of national or global economies and markets; and events and trends on a national, regional and global scale, including those of a political, economic, business, competitive and regulatory nature.', metadata={'source': 'https://www.nasdaq.com/press-release/broadcom-and-vmware-provide-update-on-pending-transaction-2023-10-30'}), Document(page_content='About Broadcom\\n\\nAbout Broadcom\\n\\nAbout\\xa0VMware\\n\\nAbout\\xa0VMware\\n\\nwww.VMware.com/company.\\n\\nCautionary Statement Regarding Forward-Looking Statements\\n\\nCautionary Statement Regarding Forward-Looking Statements\\n\\nThese forward-looking statements involve risks and uncertainties that are outside Broadcom\\'s and\\xa0VMware\\'s\\xa0control and may cause actual results to differ materially from those contained in forward-looking statements, including but not limited to: business disruption following the announcement and closing of the proposed transaction; difficulties in retaining and hiring key personnel and employees due to the proposed transaction and business combination; the diversion of management time on transaction-related issues; the satisfaction of the conditions precedent to completion of the proposed transaction, including the ability to secure regulatory approvals on the terms expected, at all or in a timely manner; the disruption of current plans and operations; the outcome of legal proceedings related to the Transaction; the ability to complete the proposed transaction on a timely basis or at all; the ability to successfully integrate\\xa0VMware\\'s\\xa0operations; cyber-attacks, information security and data privacy; global political and economic conditions, including cyclicality in the semiconductor industry and in Broadcom\\'s other target markets, rising interest rates, the impact of inflation and challenges in manufacturing and the global supply chain; the impact of public health crises, such as pandemics (including COVID-19) and epidemics and any related company or government policies and actions to protect the health and safety of individuals or government policies or actions to maintain the functioning of national or global economies and markets; and events and trends on a national, regional and global scale, including those of a political, economic, business, competitive and regulatory nature.\\n\\nThese risks, as well as other risks related to the proposed transaction, are included in the registration statement on Form S-4 and proxy statement/prospectus that has been filed with the\\xa0Securities and Exchange Commission\\xa0(\"SEC\") in connection with the proposed transaction. While the list of factors presented here is, and the list of factors presented in the registration statement on Form S-4 are, considered representative, no such list should be considered to be a complete statement of all potential risks and uncertainties. For additional information about other factors that could cause actual results to differ materially from those described in the forward-looking statements, please refer to Broadcom\\'s and\\xa0VMware\\'s\\xa0respective periodic reports and other filings with the\\xa0SEC, including the risk factors identified in Broadcom\\'s and\\xa0VMware\\'s\\xa0most recent Quarterly Reports on Form 10-Q and Annual Reports on Form 10-K. The forward-looking statements included in this communication are made only as of the date hereof. Neither Broadcom nor\\xa0VMware\\xa0undertakes any obligation to update any forward-looking statements to reflect subsequent events or circumstances, except as required by law.\\n\\nNo Offer or Solicitation\\n\\nNo Offer or Solicitation\\n\\nAdditional Information about the Transaction and Where to Find It\\n\\nAdditional Information about the Transaction and Where to Find It\\n\\nhttp://www.sec.gov. Copies of the documents filed with the\\xa0SEC\\xa0by Broadcom may be obtained free of charge on Broadcom\\'s website at\\n\\nhttps://investors.broadcom.com. Copies of the documents filed with the\\xa0SEC\\xa0by\\xa0VMware\\xa0may be obtained free of charge on\\xa0VMware\\'s\\xa0website at\\n\\nir.vmware.com.\\n\\nContacts:\\n\\nBroadcom Inc.\\n\\nBroadcom Inc.\\n\\nBroadcom Investor Relations\\n\\n408-433-8000\\n\\ninvestor.relations@broadcom.com\\n\\n(AVGO-Q)\\n\\nJoele Frank\\xa0/\\xa0Tim Ragones\\xa0/\\xa0Arielle Rothstein\\n\\nJoele Frank,\\xa0Wilkinson Brimmer Katcher\\n\\n212-355-4449\\n\\nVMware, Inc.\\n\\nVMware, Inc.\\n\\nVMware Investor Relations\\n\\nir@vmware.com\\n\\nDoreen Ruyak\\n\\nVMware Global PR\\n\\ndruyak@vmware.com', metadata={'source': 'https://news.vmware.com/releases/broadcom-and-vmware-provide-update-on-pending-transaction'}), Document(page_content='Business Wire\\n\\nBroadcom and VMware Provide Update on Pending Transaction\\n\\nBusiness Wire\\n\\n8 min read\\n\\nCompanies Will Close Transaction Promptly Following Satisfaction of Remaining Condition\\n\\nSAN JOSE, Calif. & PALO ALTO, Calif., October 30, 2023--(BUSINESS WIRE)--Broadcom Inc. (NASDAQ: AVGO) and VMware, Inc. (NYSE: VMW) today announced their expectation that Broadcom’s acquisition of VMware (the \"Transaction\") will close soon, but in any event prior to the expiration of their merger agreement.\\n\\nThe parties have received legal merger clearance in Australia, Brazil, Canada, the European Union, Israel, Japan, South Africa, South Korea, Taiwan, the United Kingdom, and foreign investment control clearance in all necessary jurisdictions. There is no legal impediment to closing under U.S. merger regulations.\\n\\nThe parties also announced the results of the elections made by VMware stockholders of record regarding the form of consideration they wish to receive in exchange for their shares of VMware common stock in connection with the Transaction. As previously disclosed, the deadline to have made such an election was 5:00 p.m. Eastern Time on October 23, 2023 (the \"Election Deadline\").\\n\\nAs further described in the election materials and in the parties’ proxy statement/prospectus dated October 3, 2022, each VMware stockholder will be entitled to receive, for each share of VMware common stock held immediately prior to the closing of the Transaction, (i) $142.50 in cash, without interest (the cash consideration), or (ii) 0.2520 of a share of Broadcom common stock (the stock consideration). The merger consideration is subject to proration so that 50% of the aggregate shares of VMware common stock outstanding immediately prior to the closing of the Transaction will be converted into the cash consideration and the remaining 50% of the aggregate shares of VMware common stock will be converted into the stock consideration. Each VMware stockholder will receive cash in lieu of any fractional shares of Broadcom common stock that the stockholder otherwise would be entitled to receive.\\n\\nBased on the final results of the merger consideration election:\\n\\nVMware stockholders of record of approximately 96% of the outstanding shares of VMware common stock elected to receive the stock consideration and in accordance with the proration procedures in the parties’ merger agreement, (i) approximately 52.1% of such outstanding shares of VMware common stock will be converted into the right to receive 0.2520 of a share of Broadcom common stock per share of VMware common stock and (ii) approximately 47.9% of such outstanding shares of VMware common stock will be converted into the right to receive $142.50 in cash per share of VMware common stock; and\\n\\nVMware stockholders of record of approximately 4% of the outstanding shares of VMware common stock elected to receive the cash consideration or did not make a valid election or did not deliver a valid election form prior to the Election Deadline. Each such VMware stockholder will be entitled to receive $142.50 in cash per share of VMware common stock.\\n\\nA more detailed description of the merger consideration and the allocation and proration procedures applicable to elections is contained in the proxy statement/prospectus. VMware stockholders are urged to read the proxy statement/prospectus carefully and in its entirety. Copies of the proxy statement/prospectus may be obtained free of charge by following the instructions below, under \"Additional Information about the Transaction and Where to Find It.\"\\n\\nAbout Broadcom', metadata={'source': 'https://finance.yahoo.com/news/broadcom-vmware-pending-transaction-090200559.html'})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## <b>EVALUATION</b>\n",
        "\n",
        "---\n",
        "\n",
        "For this we will use the 4 eval metrics provided by RAGS:\n",
        "1. Faithfulness\n",
        "2. Answer Relevancy\n",
        "3. Context Relevancy (precision)\n",
        "4. Context Recall\n",
        "\n",
        "NOTE: If we want to use certain metrics (recall) we also need to provide the ground truth answer when building the QA chain. **`ground_truths`** in the `qa_chain` call\n",
        "\n",
        "---\n",
        "\n",
        "For this first few we will use off the shelf stuff to experiment. I'll throw a cell at the end so you can put your own question and answer though."
      ],
      "metadata": {
        "id": "bGPosKl0UPRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make eval chains\n",
        "eval_chains = {\n",
        "    m.name: RagasEvaluatorChain(metric=m)\n",
        "    for m in [faithfulness, answer_relevancy, context_relevancy, context_recall]\n",
        "}\n",
        "\n",
        "# testing it out\n",
        "question = \"Who is the current CEO of OpenAI?\"\n",
        "answer = \"Emmett Shear is the CEO of OpenAI\"\n",
        "print(\"\\n\\nLLM GENERATION:\")\n",
        "llm = create_openai_model(temperature=0.5)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm, retriever=query_to_vs(question).as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "result = qa_chain({\"query\": question, \"ground_truths\":answer})\n",
        "print(\"\\n\\nSOURCES:\")\n",
        "for doc in result['source_documents']: print(doc.metadata[\"source\"])\n",
        "model_result = result[\"result\"]\n",
        "\n",
        "# evaluate\n",
        "print(\"\\n\\nRAGAS EVALUATION\")\n",
        "for name, eval_chain in eval_chains.items():\n",
        "    score_name = f\"{name}_score\"\n",
        "    print(f\"\\t{score_name}: {eval_chain(result)[score_name]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhu97ueLSN-z",
        "outputId": "7f9b7acf-ae0d-4307-b20a-fa046d54d10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "LLM GENERATION:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:07<00:00,  1.94s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current CEO of OpenAI is Emmett Shear, the former Twitch CEO. He took over as interim CEO after Sam Altman was fired.\n",
            "\n",
            "SOURCES:\n",
            "https://nymag.com/intelligencer/2023/11/why-was-sam-altman-fired-as-ceo-of-openai.html\n",
            "https://openai.com/blog/openai-announces-leadership-transition\n",
            "https://nymag.com/intelligencer/2023/11/why-was-sam-altman-fired-as-ceo-of-openai.html\n",
            "https://www.washingtonpost.com/technology/2023/11/20/openai-sam-altman-ceo-oust/\n",
            "\n",
            "\n",
            "RAGAS EVALUATION\n",
            "\tfaithfulness_score: 1.0\n",
            "\tanswer_relevancy_score: 0.9509531042041587\n",
            "\tcontext_relevancy_score: 0.010309278350515464\n",
            "\tcontext_recall_score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE: I don't know what's up with the context relevancy score... it seems off to me**\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "aygEBhsmViPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# testing it out\n",
        "question = \"Explain Quantum Tunneling in a simple way to help me understand.\"\n",
        "\n",
        "# From reddit ELI5\n",
        "answer = \"\"\"Imagine a well of sorts: there's barriers of some height, let's say ten feet. You're inside the well, and you drop (not throw, just drop) a ball from as high as your arm will reach, maybe six or seven feet. The ball will bounce around off the walls and stuff, but it will never leave the well.\n",
        "In physics, the way we justify saying that the ball will never leave is that it doesn't have enough energy: it takes a certain amount of energy to get higher up than ten feet, and you only gave the ball enough energy to get up to six feet or so. Formally, we would call the walls of the well a \"potential barrier,\" meaning that they prevent anything that doesn't have enough energy from going past them.\n",
        "This is all well and good in the case of a ball and a well, but when we get down to really small scales, things start being described by quantum mechanics instead of classical mechanics, which are the physics of the regular world that you're used to.\n",
        "In quantum mechanics, the phrasing of the problem would still be the same: there's some well, and its walls form a potential barrier, and there's a \"ball\" which we now call a \"particle\" that has a certain amount of energy. If the particle has enough energy that it can go out of the well (let's say the walls are \"10 units of energy high\" and the particle has \"20 units of energy\"), it can easily go over; that's what you would intuitively expect. The analogy to the ball and well would be if you threw the ball upwards, it's not hard to imagine that it could easily leave the well.\n",
        "That's not very interesting, though, so let's consider what happens when the particle has less energy than the height of the \"walls\" should allow. To make a huge oversimplification for the purposes of the explanation, in quantum mechanics, everything behaves sort of like a wave, at least from a mathematical point of view. When we do the math, we find that the formulas for how the particle in the well behaves as it hits the barrier are pretty similar to the way a wave behaves when it hits a different material.\n",
        "So, let's think about light (which is sort of a wave) hitting a pane of glass. The light comes in, some of it goes through the glass, and some of it reflects. In fancy terms, there's an incident (incoming) wave, a transmitted (goes through) wave, and a reflected wave.\n",
        "The specifics of the problem (in this case, the energies involved) and a lot of math tells us how much of the wave gets transmitted, which, in the case of quantum tunneling, means that some of the \"wave\" that the particle consists of actually goes through the barrier, while the rest gets reflected and stays inside the well.\n",
        "Since some of the \"wave\" has gone through the barrier, what that means in a physical sense is that there's a chance that we'll find the particle on the other side when we measure it. The \"wave\" (more formally, wave function) is a mathematical construct that tells us how likely we are to find the particle in a certain place, so if some of the wave is on the other side of the barrier, there is a real possibility that we will end up finding the particle there.\"\"\"\n",
        "llm = create_openai_model(temperature=0.5)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm, retriever=query_to_vs(question).as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "result = qa_chain({\"query\": question, \"ground_truths\":answer})\n",
        "model_result = result[\"result\"]\n",
        "\n",
        "\n",
        "# evaluate\n",
        "print(\"\\n\\nRAGAS EVALUATION\")\n",
        "for name, eval_chain in eval_chains.items():\n",
        "    score_name = f\"{name}_score\"\n",
        "    print(f\"\\t{score_name}: {eval_chain(result)[score_name]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfFf3jffnkML",
        "outputId": "9bf6c988-06bb-4dfb-cb6a-6fe7a745c56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:04<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantum tunneling is a phenomenon that occurs in the world of quantum mechanics, the science that describes how tiny particles like atoms and photons behave. In simple terms, it's like this: Imagine a ball that you're trying to roll up a hill, but the ball doesn't have enough energy to get all the way to the top. According to the laws of classical physics, the ball would roll back down. \n",
            "\n",
            "But in quantum mechanics, there's a chance that the ball could suddenly appear on the other side of the hill, as if it had tunneled through the hill. This is quantum tunneling - a particle can pass through a barrier that it shouldn't be able to according to classical physics. \n",
            "\n",
            "This happens because, in the quantum world, particles can behave like waves. And just like how a wave can spread out and extend beyond a barrier, a particle's \"wave function\" (which describes its location) can also extend beyond a barrier. This means there's a chance the particle can be found on the other side of the barrier. \n",
            "\n",
            "This might sound strange, but quantum tunneling is actually a fundamental part of many technologies we use today, like transistors in electronics, which rely on electrons tunneling through barriers.\n",
            "\n",
            "RAGAS EVALUATION\n",
            "\tfaithfulness_score: 1.0\n",
            "\tanswer_relevancy_score: 0.9054829360337665\n",
            "\tcontext_relevancy_score: 0.07228915662650602\n",
            "\tcontext_recall_score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing it out where it may fail\n",
        "question = \"What art show did the painter Kristi McDonald last attend?\"\n",
        "answer = \"The Artist Project in Toronto, Ontario, Canada\"\n",
        "llm = create_openai_model(temperature=0.5)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm, retriever=query_to_vs(question).as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "result = qa_chain({\"query\": question, \"ground_truths\":answer})\n",
        "model_result = result[\"result\"]\n",
        "\n",
        "\n",
        "# evaluate\n",
        "print(\"\\n\\nRAGAS EVALUATION\")\n",
        "for name, eval_chain in eval_chains.items():\n",
        "    score_name = f\"{name}_score\"\n",
        "    print(f\"\\t{score_name}: {eval_chain(result)[score_name]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmI6oMa9tuaT",
        "outputId": "b63f7049-9a22-4d5f-e485-df88bcab42b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text doesn't provide information on a painter named Kristi McDonald or any art shows she may have attended.\n",
            "\n",
            "RAGAS EVALUATION\n",
            "\tfaithfulness_score: 0.0\n",
            "\tanswer_relevancy_score: 0.9402344131399345\n",
            "\tcontext_relevancy_score: 0.0\n",
            "\tcontext_recall_score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing it out where there are two answers and the answer is 50% incorrect\n",
        "question = \"What is the heaviest element and the lightest element in the periodic table??\"\n",
        "answer = \"The lightest element is Hydrogen and the heaviest element is Hassium.\"\n",
        "llm = create_openai_model(temperature=0.5)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm, retriever=query_to_vs(question).as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "result = qa_chain({\"query\": question, \"ground_truths\":answer})\n",
        "model_result = result[\"result\"]\n",
        "\n",
        "\n",
        "# evaluate\n",
        "print(\"\\n\\nRAGAS EVALUATION\")\n",
        "for name, eval_chain in eval_chains.items():\n",
        "    score_name = f\"{name}_score\"\n",
        "    print(f\"\\t{score_name}: {eval_chain(result)[score_name]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT5rIBjkuLyg",
        "outputId": "1796d7a2-c82f-42d9-8eec-b068744966e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:04<00:00,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lightest element in the periodic table is Hydrogen with an atomic number of 1. The heaviest element that has been officially recognized is Oganesson, with an atomic number of 118.\n",
            "\n",
            "RAGAS EVALUATION\n",
            "\tfaithfulness_score: 0.75\n",
            "\tanswer_relevancy_score: 0.9206485077548664\n",
            "\tcontext_relevancy_score: 0.005\n",
            "\tcontext_recall_score: 0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing it out where the answer is corrected\n",
        "question = \"What is the heaviest element and the lightest element in the periodic table??\"\n",
        "answer = \"The lightest element is Hydrogen and the heaviest element is Oganesson.\"\n",
        "llm = create_openai_model(temperature=0.5)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm, retriever=query_to_vs(question).as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "result = qa_chain({\"query\": question, \"ground_truths\":answer})\n",
        "model_result = result[\"result\"]\n",
        "\n",
        "\n",
        "# evaluate\n",
        "print(\"\\n\\nRAGAS EVALUATION\")\n",
        "for name, eval_chain in eval_chains.items():\n",
        "    score_name = f\"{name}_score\"\n",
        "    print(f\"\\t{score_name}: {eval_chain(result)[score_name]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS4gFNJ9ug4X",
        "outputId": "361ce0d9-f0ac-4e24-f7ca-7191d1b494cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lightest element in the periodic table is Hydrogen, which has only one proton in its atomic nucleus. The heaviest element that has been named and recognized is Oganesson, which has 118 protons in its atomic nucleus. However, heavier synthetic elements have been produced in laboratories.\n",
            "\n",
            "RAGAS EVALUATION\n",
            "\tfaithfulness_score: 0.8\n",
            "\tanswer_relevancy_score: 0.9326742963134859\n",
            "\tcontext_relevancy_score: 0.034782608695652174\n",
            "\tcontext_recall_score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing it out where the answer is corrected\n",
        "question = \"What are all of the intransitive definitions for the word run?\"\n",
        "answer = \"\"\"Run (verb, intransitive): To move quickly by lifting the feet rapidly, often with moments where both feet are off the ground.\n",
        "Run (verb, intransitive): To move or act swiftly; hurry.\n",
        "Run (verb, intransitive): To leave quickly; flee or escape.\n",
        "Run (verb, intransitive): To seek help or comfort.\n",
        "Run (verb, intransitive): To make a quick trip or visit.\n",
        "Run (verb, intransitive): To move about without restraint.\n",
        "Run (verb, intransitive): To continue movement from momentum.\n",
        "Run (verb, intransitive, sports): To participate in a race.\n",
        "Run (verb, intransitive, sports): To finish a race in a certain position.\n",
        "Run (verb, intransitive): To campaign for election.\n",
        "Run (verb, intransitive): To migrate, as in fish.\n",
        "Run (verb, intransitive): To operate or function, as in machinery.\n",
        "Run (verb, intransitive): To deviate from a course.\n",
        "Run (verb, intransitive): To operate on a route, as in transportation.\n",
        "Run (verb, intransitive): To move smoothly or freely.\n",
        "Run (verb, intransitive): To spread or extend, as in vines.\n",
        "Run (verb, intransitive): To unravel, as in fabric.\n",
        "Run (verb, intransitive): To flow, as in liquids.\n",
        "Run (verb, intransitive): To melt and flow, as in wax.\n",
        "Run (verb, intransitive, golf): To bounce or roll after landing.\n",
        "Run (verb, intransitive): To spread when applied.\n",
        "Run (verb, intransitive): To discharge a liquid.\n",
        "Run (verb, intransitive): To have a range of variations.\n",
        "Run (verb, intransitive, commerce): To accumulate interest.\n",
        "Run (verb, intransitive, law): To have legal effect.\"\"\"\n",
        "\n",
        "llm = create_openai_model(temperature=0.5)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm, retriever=query_to_vs(question).as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "result = qa_chain({\"query\": question, \"ground_truths\":answer})\n",
        "model_result = result[\"result\"]\n",
        "\n",
        "\n",
        "# evaluate\n",
        "print(\"\\n\\nRAGAS EVALUATION\")\n",
        "for name, eval_chain in eval_chains.items():\n",
        "    score_name = f\"{name}_score\"\n",
        "    print(f\"\\t{score_name}: {eval_chain(result)[score_name]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOnbc82lu6t6",
        "outputId": "3dddc2c1-f56c-4d2a-cf24-3a6a145ba079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As an intransitive verb, \"run\" can have several meanings. Here are some of them:\n",
            "\n",
            "1. To move swiftly on foot: \"He runs every morning for exercise.\"\n",
            "2. To flee; to take to flight; to escape: \"When they saw the police, they ran.\"\n",
            "3. To compete in a race: \"She is going to run in the marathon.\"\n",
            "4. To flow, like liquid or colors: \"The river runs through the city\" or \"The colors in the shirt ran when it was washed.\"\n",
            "5. To operate or function: \"The machine runs on electricity.\"\n",
            "6. To continue in time: \"The play runs for two hours.\"\n",
            "7. To be in charge of; manage or direct: \"She runs a small business.\"\n",
            "8. To be a candidate for public office: \"He is going to run for mayor.\"\n",
            "\n",
            "Please note that the exact definitions can vary a bit depending on the dictionary used.\n",
            "\n",
            "RAGAS EVALUATION\n",
            "\tfaithfulness_score: 0.0\n",
            "\tanswer_relevancy_score: 0.9326056462489393\n",
            "\tcontext_relevancy_score: 0.010714285714285714\n",
            "\tcontext_recall_score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing it out where the answer is corrected\n",
        "question = \"What is today's date and time if you live in India if it is 6:25 PM on November 18, 2023\"\n",
        "answer = \"November 19, 2023 - 4:55 AM\"\n",
        "llm = create_openai_model(temperature=0.5)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm, retriever=query_to_vs(question).as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "result = qa_chain({\"query\": question, \"ground_truths\":answer})\n",
        "model_result = result[\"result\"]\n",
        "\n",
        "\n",
        "# evaluate\n",
        "print(\"\\n\\nRAGAS EVALUATION\")\n",
        "for name, eval_chain in eval_chains.items():\n",
        "    score_name = f\"{name}_score\"\n",
        "    print(f\"\\t{score_name}: {eval_chain(result)[score_name]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fdbgdP3v_vD",
        "outputId": "83445024-e2f5-4d1a-b44b-558b1275f0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The date and time in India would be 11:55 PM on November 18, 2023. This is because Indian Standard Time is UTC+05:30, so it is 5 hours and 30 minutes ahead.\n",
            "\n",
            "RAGAS EVALUATION\n",
            "\tfaithfulness_score: 0.0\n",
            "\tanswer_relevancy_score: 0.8345881689419042\n",
            "\tcontext_relevancy_score: 0.01444043321299639\n",
            "\tcontext_recall_score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## <b>AGENT FUN</b>\n",
        "\n",
        "---\n",
        "\n",
        "Let's make a basic agent that only has access to two tools\n",
        "1. Our QARetrieval chain as a tool\n",
        "2. An LLM performing review that will check if our answer is any good...\n",
        "\n",
        "Here's a picture to showcase the basics of an agent...\n",
        "\n",
        "<center><img src=\"https://lilianweng.github.io/posts/2023-06-23-agent/agent-overview.png\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "hqWpA2c_Vu_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "def rag_tool(query, return_src=False, temperature=0.5):\n",
        "    llm = create_openai_model(temperature=temperature)\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm, retriever=query_to_vs(query).as_retriever(),\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    result = qa_chain({\"query\": query})\n",
        "    return result[\"result\"]\n",
        "\n",
        "review_prompt = \"\"\"You are an expert reviewer.\n",
        "Your task is to see if the returned answer completely addresses the original question posed by the user.\n",
        "You are not responsible for determining the correctness of the response, instead, you are responsible for simply determining if the answer seems complete and seems to be addressing the user's question.\n",
        "If so, you will simply return an affirmitive. Otherwise, return an explanation of what you feel like is missing.\n",
        "You can also suggest a query or queries that may be useful in acquiring additional helpful information.\n",
        "\n",
        "You are an expert and will do a great job. Remember, it's ok to simply say the answer did a good job and leave it at that.\n",
        "\n",
        "---\n",
        "\n",
        "QUESTION AND ANSWER: {question_and_answer}\n",
        "REVIEW: \"\"\"\n",
        "\n",
        "# Initiate our LLM - default is 'gpt-3.5-turbo'\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "\n",
        "# LLM chain consisting of the LLM and a prompt\n",
        "llm_chain = LLMChain(llm=llm, prompt=custom_agent_prompt)\n",
        "\n",
        "\n",
        "def review_tool(question_and_answer):\n",
        "    llm = create_openai_model(temperature=0.0)\n",
        "    review_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(review_prompt))\n",
        "    return review_chain({\"question_and_answer\": question_and_answer})\n",
        "\n",
        "tools = [\n",
        "    # rag_tool\n",
        "    Tool(\n",
        "        name=\"RAG Tool\",\n",
        "        func=lambda x: rag_tool(query=x),\n",
        "        description=\"useful for whenever you need to answer a user's question as this uses external expert sources\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Review Tool\",\n",
        "        func=lambda x: review_tool(question_and_answer=x),\n",
        "        description=\"useful for whenever you want to review if the generated answer addressed the user's question. The input to this tool is the concatenation of the original user input and the generated answer\"\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "V5NsWSq6Y43W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
        "from typing import List\n",
        "\n",
        "# Set up the prompt with input variables for tools, user input and a scratchpad for the model to record its workings\n",
        "# Set up a prompt template which can interpolate the history\n",
        "template_with_history = \"\"\"You are SearchGPT, a professional search engine who provides informative answers to users. Answer the following questions as best you can. If you need to display an image then use the Python REPL tool. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "You must always start by using the RAG tool followed by the Review tool.\n",
        "Try to only use the Review tool once.\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin! Remember to give detailed, informative answers\n",
        "\n",
        "Previous conversation history:\n",
        "{history}\n",
        "\n",
        "New question: {input}\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "# Set up a prompt template\n",
        "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
        "    # The template to use\n",
        "    template: str\n",
        "    # The list of tools available\n",
        "    tools: List[BaseTool]\n",
        "\n",
        "    def format_messages(self, **kwargs) -> str:\n",
        "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
        "\n",
        "        # Format them in a particular way\n",
        "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
        "        thoughts = \"\"\n",
        "        for action, observation in intermediate_steps:\n",
        "            thoughts += action.log\n",
        "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
        "\n",
        "        # Set the agent_scratchpad variable to that value\n",
        "        kwargs[\"agent_scratchpad\"] = thoughts\n",
        "\n",
        "        # Create a tools variable from the list of tools provided\n",
        "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
        "\n",
        "        # Create a list of tool names for the tools provided\n",
        "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
        "        formatted = self.template.format(**kwargs)\n",
        "        return [HumanMessage(content=formatted)]\n",
        "\n",
        "custom_agent_prompt = CustomPromptTemplate(\n",
        "    template=template_with_history,\n",
        "    tools=tools,\n",
        "    # The history template includes \"history\" as an input variable so we can interpolate it into the prompt\n",
        "    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
        ")\n",
        "\n",
        "print(custom_agent_prompt.format_messages(input=\"Placeholder for user input\", history=\"history goes here\", intermediate_steps=[])[0].content)"
      ],
      "metadata": {
        "id": "ZAYBYWKjwWgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f6a632f-97bd-441f-bcbe-982da5cba368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are SearchGPT, a professional search engine who provides informative answers to users. Answer the following questions as best you can. If you need to display an image then use the Python REPL tool. You have access to the following tools:\n",
            "\n",
            "RAG Tool: useful for whenever you need to answer a user's question as this uses external expert sources\n",
            "Review Tool: useful for whenever you want to review if the generated answer addressed the user's question. The input to this tool is the concatenation of the original user input and the generated answer\n",
            "\n",
            "You must always start by using the RAG tool followed by the Review tool.\n",
            "Try to only use the Review tool once.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [RAG Tool, Review Tool]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin! Remember to give detailed, informative answers\n",
            "\n",
            "Previous conversation history:\n",
            "history goes here\n",
            "\n",
            "New question: Placeholder for user input\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union\n",
        "\n",
        "class CustomOutputParser(AgentOutputParser):\n",
        "    \"\"\"\n",
        "    A custom parser for interpreting the output of an LLM system.\n",
        "    This class is a child of the `AgentOutputParser` class.\n",
        "\n",
        "    methods:\n",
        "        parse(llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "            - Processes a string output from an LLM system and returns\n",
        "              an AgentAction or AgentFinish object based on the output content.\n",
        "    \"\"\"\n",
        "\n",
        "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "        \"\"\"\n",
        "        Parses the given LLM output and returns an AgentAction or AgentFinish object.\n",
        "        The LLM output is expected to contain either a \"Final Answer:\" message or\n",
        "        an action and action input in the format \"Action: [action]\\nAction Input: [input]\".\n",
        "        If the \"Final Answer:\" message is present, an AgentFinish object is returned.\n",
        "        Otherwise, an AgentAction object is returned containing the extracted action and input.\n",
        "\n",
        "        Args:\n",
        "            llm_output (str)\n",
        "                - The output string from the LLM system.\n",
        "\n",
        "        Returns:\n",
        "            - The AgentAction or AgentFinish object based on the LLM output content.\n",
        "            - The returned value will be of type Union[AgentAction, AgentFinish]\n",
        "\n",
        "        Raises:\n",
        "            ValueError\n",
        "                - If the LLM output doesn't contain a recognizable action and action input.\n",
        "\n",
        "        Example Usage\n",
        "        -------\n",
        "        >>> parser = CustomOutputParser()\n",
        "        >>> parser.parse(\"Action: exampleAction\\nAction Input: exampleInput\")\n",
        "        <AgentAction object with tool=\"exampleAction\", tool_input=\"exampleInput\">\n",
        "        \"\"\"\n",
        "\n",
        "        # Check for a 'Final Answer:' in the LLM output\n",
        "        if \"Final Answer:\" in llm_output:\n",
        "            # Extract everything after 'Final Answer:' and remove leading/trailing white spaces\n",
        "            final_answer = llm_output.split(\"Final Answer:\")[-1].strip()\n",
        "\n",
        "            return AgentFinish(\n",
        "                # Return the final answer in a dictionary\n",
        "                return_values={\"output\": final_answer},\n",
        "                log=llm_output,\n",
        "            )\n",
        "\n",
        "        # Regular expression pattern for extracting action and action input\n",
        "        #     - this regex is designed to match a string that starts with \"Action: \",\n",
        "        #     - followed by any characters (captured),\n",
        "        #     - followed by any number of newlines,\n",
        "        #     - then \"Action Input:\", and\n",
        "        #     - then any amount of whitespace, and\n",
        "        #     - finally, any characters (captured).\n",
        "        regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n",
        "\n",
        "        # The re.search() function scans through the string, looking for any location where\n",
        "        # this pattern matches. If it finds a match, it returns a match object.\n",
        "        # Otherwise, it returns None.\n",
        "        #\n",
        "        # The re.DOTALL flag means that the `.` special character can match\n",
        "        # any character including a newline, which it normally does not match.\n",
        "        match = re.search(regex, llm_output, re.DOTALL)\n",
        "\n",
        "        # If no action and action input are found, raise a ValueError\n",
        "        if not match:\n",
        "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
        "\n",
        "        # Extract the action (group 1) and action input (group 2) from the match object\n",
        "        action = match.group(1).strip()\n",
        "        action_input = match.group(2)\n",
        "\n",
        "        # Remove leading/trailing spaces and quotation marks from action input\n",
        "        action_input = action_input.strip(\" \").strip('\"')\n",
        "\n",
        "        # Return the action and action input in an AgentAction object\n",
        "        return AgentAction(tool=action, tool_input=action_input, log=llm_output)\n",
        "\n",
        "output_parser = CustomOutputParser()\n",
        "print(\"Action: REPL\\nAction Input:print(hello world)\")\n",
        "print(output_parser.parse(\"Action: REPL\\nAction Input:print(hello world)\"))\n",
        "\n",
        "print(\"Action: REPL\\nAction Input: print(hello world)\")\n",
        "print(output_parser.parse(\"Action: REPL\\nAction Input: print(hello world)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azmA8fuPYR2o",
        "outputId": "c0ef2f96-61a7-440f-b157-0d5048aa5a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action: REPL\n",
            "Action Input:print(hello world)\n",
            "tool='REPL' tool_input='print(hello world)' log='Action: REPL\\nAction Input:print(hello world)'\n",
            "Action: REPL\n",
            "Action Input: print(hello world)\n",
            "tool='REPL' tool_input='print(hello world)' log='Action: REPL\\nAction Input: print(hello world)'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# Initiate our LLM - default is 'gpt-3.5-turbo'\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "\n",
        "# LLM chain consisting of the LLM and a prompt\n",
        "llm_chain = LLMChain(llm=llm, prompt=custom_agent_prompt)\n",
        "\n",
        "# Using tools, the LLM chain and output_parser to make an agent\n",
        "tool_names = [tool.name for tool in tools]\n",
        "\n",
        "agent = LLMSingleActionAgent(\n",
        "    llm_chain=llm_chain,\n",
        "    output_parser=output_parser,\n",
        "    stop=[\"\\nObservation:\"],\n",
        "    allowed_tools=tool_names\n",
        ")\n",
        "\n",
        "# Initiate the memory with k=2 to keep the last two turns\n",
        "# Provide the memory to the agent\n",
        "# Initiate the agent that will respond to our queries\n",
        "# Set verbose=True to share the CoT reasoning the LLM goes through\n",
        "memory = ConversationBufferWindowMemory(k=3)\n"
      ],
      "metadata": {
        "id": "eg8F25Mib-dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, memory=memory)\n",
        "agent_executor.run(\n",
        "    \"Tell me about the drama with OpenAI? \"\\\n",
        "    \"Who is the current CEO? \" \\\n",
        "    \"Who was the CEO on November 18th? \" \\\n",
        "    \"What about the 15th?\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "jyHn08DEcSlA",
        "outputId": "fa792238-0567-43a9-bae0-a8153bb67ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The user is asking for information about any controversies or issues involving OpenAI, as well as the current CEO and the CEO on specific dates. I will use the RAG tool to gather this information.\n",
            "\n",
            "Action: RAG Tool\n",
            "\n",
            "Action Input: Tell me about the drama with OpenAI? Who is the current CEO? Who was the CEO on November 18th? What about the 15th?\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:08<00:00,  2.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The drama at OpenAI involved the abrupt firing of CEO Sam Altman. Altman was fired by four members of the company's board, who stated that he was not consistently candid in his communications with the board, hindering its ability to exercise its responsibilities.\n",
            "\n",
            "On November 18th, the CEO was Sam Altman until he was fired. Following his dismissal, the company's CTO, Mira Murati, was named as the interim CEO. However, this was short-lived as by November 20th, the company announced that it had hired Twitch co-founder Emmett Shear as another interim CEO, making him the third chief executive in three days. Therefore, on November 15th, the CEO was still Sam Altman.\n",
            "\n",
            "Observation:\u001b[36;1m\u001b[1;3mThe drama at OpenAI involved the abrupt firing of CEO Sam Altman. Altman was fired by four members of the company's board, who stated that he was not consistently candid in his communications with the board, hindering its ability to exercise its responsibilities.\n",
            "\n",
            "On November 18th, the CEO was Sam Altman until he was fired. Following his dismissal, the company's CTO, Mira Murati, was named as the interim CEO. However, this was short-lived as by November 20th, the company announced that it had hired Twitch co-founder Emmett Shear as another interim CEO, making him the third chief executive in three days. Therefore, on November 15th, the CEO was still Sam Altman.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe RAG tool has provided a detailed answer to the user's question, including the drama at OpenAI, the current CEO, and the CEO on the specified dates. I will now use the Review tool to ensure the generated answer is accurate and addresses the user's question.\n",
            "\n",
            "Action: Review Tool\n",
            "\n",
            "Action Input: Tell me about the drama with OpenAI? Who is the current CEO? Who was the CEO on November 18th? What about the 15th? The drama at OpenAI involved the abrupt firing of CEO Sam Altman. Altman was fired by four members of the company's board, who stated that he was not consistently candid in his communications with the board, hindering its ability to exercise its responsibilities. On November 18th, the CEO was Sam Altman until he was fired. Following his dismissal, the company's CTO, Mira Murati, was named as the interim CEO. However, this was short-lived as by November 20th, the company announced that it had hired Twitch co-founder Emmett Shear as another interim CEO, making him the third chief executive in three days. Therefore, on November 15th, the CEO was still Sam Altman.\n",
            "\u001b[0mThe answer is complete and addresses all parts of the user's question.\n",
            "\n",
            "Observation:\u001b[33;1m\u001b[1;3m{'question_and_answer': \"Tell me about the drama with OpenAI? Who is the current CEO? Who was the CEO on November 18th? What about the 15th? The drama at OpenAI involved the abrupt firing of CEO Sam Altman. Altman was fired by four members of the company's board, who stated that he was not consistently candid in his communications with the board, hindering its ability to exercise its responsibilities. On November 18th, the CEO was Sam Altman until he was fired. Following his dismissal, the company's CTO, Mira Murati, was named as the interim CEO. However, this was short-lived as by November 20th, the company announced that it had hired Twitch co-founder Emmett Shear as another interim CEO, making him the third chief executive in three days. Therefore, on November 15th, the CEO was still Sam Altman.\\n\", 'text': \"The answer is complete and addresses all parts of the user's question.\"}\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe Review tool confirms that the answer generated by the RAG tool is complete and addresses all parts of the user's question.\n",
            "\n",
            "Final Answer: The drama at OpenAI involved the abrupt firing of CEO Sam Altman. Altman was fired by four members of the company's board, who stated that he was not consistently candid in his communications with the board, hindering its ability to exercise its responsibilities. On November 18th, the CEO was Sam Altman until he was fired. Following his dismissal, the company's CTO, Mira Murati, was named as the interim CEO. However, this was short-lived as by November 20th, the company announced that it had hired Twitch co-founder Emmett Shear as another interim CEO, making him the third chief executive in three days. Therefore, on November 15th, the CEO was still Sam Altman.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The drama at OpenAI involved the abrupt firing of CEO Sam Altman. Altman was fired by four members of the company's board, who stated that he was not consistently candid in his communications with the board, hindering its ability to exercise its responsibilities. On November 18th, the CEO was Sam Altman until he was fired. Following his dismissal, the company's CTO, Mira Murati, was named as the interim CEO. However, this was short-lived as by November 20th, the company announced that it had hired Twitch co-founder Emmett Shear as another interim CEO, making him the third chief executive in three days. Therefore, on November 15th, the CEO was still Sam Altman.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ON LOOP**"
      ],
      "metadata": {
        "id": "IkAZNWL4fLNP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0cD24YyixxG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}